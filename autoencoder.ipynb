{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Flatten,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tansformations with OOP'''\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "    '''Class containing methods to transform the imported data'''\n",
    "\n",
    "    # in OOP do u not need to call return?\n",
    "    def transform(self):\n",
    "        '''overall transformation of data'''\n",
    "        self.interpolate()\n",
    "        self.add_cyclical_features()\n",
    "        self.add_time_features()\n",
    "        self.ohe()\n",
    "        self.add_historical_windpower()\n",
    "        self.add_momentum_force()\n",
    "        self.scale()\n",
    "        return self.df\n",
    "\n",
    "    def interpolate(self):\n",
    "        '''interpolation of data'''\n",
    "        df = self.df\n",
    "        df['Time'] = df['Time'].apply(lambda x : datetime.datetime.strptime(x[:-3], '%Y/%m/%d %H:%M'))\n",
    "        df['Time'] = pd.to_datetime(df['Time'])    #why double the time conversion?\n",
    "        df.set_index('Time',inplace=True)  \n",
    "        df = df.resample('1H').asfreq()    #unsure about resample and asfreq\n",
    "        df.interpolate(method='cubic',axis=0,limit_direction='both',inplace=True)\n",
    "        self.df = df\n",
    "\n",
    "    def add_cyclical_features(self):\n",
    "        '''converts direction into cylical inputs'''\n",
    "        df = self.df\n",
    "        cols = df.columns \n",
    "        for c in cols:\n",
    "            if 'Direction' in c:\n",
    "                df[c+'_norm'] = df[c]/360\n",
    "                df[c+'_sin'] = df[c+'_norm'].apply(lambda x: np.sin(x))\n",
    "                df[c+'_cos'] = df[c+'_norm'].apply(lambda x: np.cos(x))\n",
    "                df.drop([c,c+'_norm'],inplace=True,axis=1)\n",
    "\n",
    "        self.df = df \n",
    "\n",
    "    def scale(self):\n",
    "        '''normalize entire dataframe'''\n",
    "        df = self.df\n",
    "        df = pd.DataFrame(StandardScaler().fit_transform(df),index=df.index,columns=df.columns)\n",
    "        self.df = df\n",
    "\n",
    "    def add_time_features(self):\n",
    "        '''create time inputs as attributes?'''\n",
    "        df = self.df\n",
    "        df.reset_index(inplace=True,drop=False)\n",
    "        #this is assigment of attribute?\n",
    "        df['hour'] = df['Time'].apply(lambda x: x.hour).astype(str)\n",
    "        df['month'] = df['Time'].apply(lambda x: x.month).astype(str)\n",
    "        # df['day'] = df['Time'].apply(lambda x: x.day).astype(str)\n",
    "        df.set_index('Time',inplace=True)\n",
    "        self.df = df\n",
    "\n",
    "    def ohe(self):\n",
    "        '''One hot encoding of time data'''\n",
    "        #what is this? I assume it standings for one hot encoding\n",
    "        #doesn't it affect the entire frame vs just the select month or year?\n",
    "        df = self.df\n",
    "        df = pd.get_dummies(df)\n",
    "        self.df = df\n",
    "\n",
    "    def add_historical_windpower(self):\n",
    "        '''conversion of windspeed into windpower'''\n",
    "        df = self.df\n",
    "        t = pd.read_csv('target.csv')\n",
    "        t['Time'] = pd.to_datetime(t['Time'])\n",
    "        t.set_index('Time',inplace=True)\n",
    "        #how does this standardscaler object behave?\n",
    "        target_scaler = StandardScaler().fit(t)\n",
    "        t = pd.DataFrame(target_scaler.transform(t),index=t.index,columns=t.columns)\n",
    "        df = df.join(t,how='left')\n",
    "        self.target_scaler = target_scaler\n",
    "        self.df = df\n",
    "\n",
    "    def add_momentum_force(self):\n",
    "        '''add momentum'''\n",
    "        time_lag = 18\n",
    "        df = self.df \n",
    "        df['Wind Energy Lag {}'.format(time_lag)] = df['Wind Energy'].shift(time_lag)\n",
    "        df['Wind Energy Lag {}'.format(2*time_lag)] = df['Wind Energy'].shift(2*time_lag)\n",
    "        df.dropna(axis=0,inplace=True) ####DROPPING 10 ROWS OF DATA HERE\n",
    "        # are you not subtracting the future values from present here?\n",
    "        df['Momentum'] = df['Wind Energy'] - df['Wind Energy Lag {}'.format(time_lag)]\n",
    "        df['Force'] = df['Wind Energy'] - 2*df['Wind Energy Lag {}'.format(time_lag)] + df['Wind Energy Lag {}'.format(2*time_lag)]\n",
    "        df.drop(['Wind Energy Lag {}'.format(time_lag),'Wind Energy Lag {}'.format(2*time_lag)],axis=1,inplace=True)\n",
    "        self.df = df\n",
    "\n",
    "        ### generate lagged input\n",
    "        lagged = pd.DataFrame(df['Wind Energy'].shift(1))\n",
    "        lagged.fillna(method='bfill',inplace=True)\n",
    "        lagged = StandardScaler().fit_transform(lagged.values)\n",
    "        self.lagged_input = lagged\n",
    "        \n",
    "\n",
    "    #----GETTER Functions---\n",
    "    #what are they for?\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "\n",
    "    def get_lagged_input(self):\n",
    "        return self.lagged_input\n",
    "\n",
    "    def get_target_scaler(self):\n",
    "        return self.target_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_b_suffix(og_name):\n",
    "    return_list = []\n",
    "    for f in og_name:\n",
    "        return_list.append(f+\"-b\")\n",
    "    return return_list\n",
    "\n",
    "locations = ['guitrancourt', 'lieusaint', \n",
    "             'lvs-pussay','parc-du-gatinais', \n",
    "             'arville','boissy-la-riviere',\n",
    "             'angerville-1','angerville-2']\n",
    "\n",
    "wind_energy = 'energy-ile-de-france'\n",
    "forecast_endpt = 'https://ai4impact.org/P003/'\n",
    "analysis_endpt = 'https://ai4impact.org/P003/historical/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = locations\n",
    "model_2 = add_b_suffix(model_1)\n",
    "models = [model_1, model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 0\n",
    "for m in models:\n",
    "    model_num += 1\n",
    "    df = pd.read_csv(analysis_endpt+m[0]+'.csv',skiprows=3)\n",
    "    df.columns = ['Time','Speed_'+m[0],'Direction_'+m[0]]\n",
    "    df.set_index('Time',inplace=True)\n",
    "    for i in range(1,len(m)):\n",
    "        loc = m[i]\n",
    "        temp = pd.read_csv(analysis_endpt+loc+'.csv',skiprows=3)\n",
    "        temp.columns = ['Time','Speed_'+loc,'Direction_'+loc]\n",
    "        temp.set_index('Time',inplace=True)\n",
    "        df = df.merge(temp,how='left',on='Time')\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    df.reset_index(inplace=True,drop=False)\n",
    "    df.to_csv(f'model_{model_num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('model_1.csv')\n",
    "df2 = pd.read_csv('model_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('https://ai4impact.org/P003/historical/energy-ile-de-france.csv',header=None)\n",
    "target.columns = ['Time','Wind Energy']\n",
    "target.to_csv('target.csv',index=False)\n",
    "target['Time'] = pd.to_datetime(target['Time'])\n",
    "target.set_index('Time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-61138568a0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lagged_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df = df1.copy()\n",
    "transformer = DataTransformer(df)\n",
    "transformer.transform()\n",
    "df = transformer.get_df()\n",
    "lagged = transformer.get_lagged_input()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31765, 64), (31765,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.loc[:,df.columns!='Wind Energy'].values\n",
    "y = df['Wind Energy'].values\n",
    "X = np.concatenate((X,lagged),axis=1)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19059, 62), (12706, 62), (19059,), (12706,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "x_train_features = x_train[:,:62]\n",
    "x_test_features = x_test[:,:62]\n",
    "x_train_lagged = x_train[:,62]\n",
    "x_test_lagged = x_test[:,62]\n",
    "x_train_features.shape,x_test_features.shape,x_train_lagged.shape,x_test_lagged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "features = Input(shape = (62,), name = 'Features')\n",
    "def create_encoder(features, latent_dim = latent_dim):\n",
    "    features = Input(shape = (62,), name = 'Features')\n",
    "    X = Dense(62, activation = 'relu')(features)\n",
    "    X = Dense(32, activation = 'relu')(X)\n",
    "    LATENT = Dense(latent_dim, activation = 'relu')(X)\n",
    "    encoder = Model(features, LATENT, name = 'encoder')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(latent_dim = latent_dim):\n",
    "    LATENT_INPUTS = Input(shape = (latent_dim,))\n",
    "    X = Dense(32, activation = 'relu')(LATENT_INPUTS)\n",
    "    OUTPUTS = Dense(62, activation = 'linear')(X)\n",
    "    decoder = Model(LATENT_INPUTS, OUTPUTS, name = 'decoder')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference Model\n",
    "def energy(latent_dim = latent_dim):\n",
    "    LATENT_INPUTS = Input(shape = (latent_dim,))\n",
    "    X = Dense(64,activation='relu')(LATENT_INPUTS)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(32,activation='relu')(X)\n",
    "    X = Dense(6,activation='relu')(X)\n",
    "\n",
    "    output = Dense(1,activation='linear')(X)\n",
    "    model = Model(inputs= LATENT_INPUTS ,outputs=output, name = 'energy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Features:0' shape=(None, 62) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_encoder(62)\n",
    "decoder = create_decoder()\n",
    "energy = energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(features, \n",
    "                    decoder(encoder(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Features (InputLayer)        [(None, 62)]              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 16)                6450      \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 62)                2590      \n",
      "=================================================================\n",
      "Total params: 9,040\n",
      "Trainable params: 9,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(features):\n",
    "    full_model = Model(inputs = features, outputs = [decoder(encoder(features)), energy(encoder(features))],\n",
    "                       name = 'complete')\n",
    "    full_model.compile(loss = {'energy': 'mse',\n",
    "                              'decoder': 'mse'},\n",
    "                      optimizer = 'adam')\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = full_model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"complete\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Features (InputLayer)           [(None, 62)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 16)           6450        Features[0][0]                   \n",
      "                                                                 Features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 62)           2590        encoder[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "energy (Model)                  (None, 1)            3373        encoder[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,413\n",
      "Trainable params: 12,413\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.9243 - decoder_loss: 0.6953 - energy_loss: 0.2290 - val_loss: 0.7525 - val_decoder_loss: 0.5896 - val_energy_loss: 0.1629\n",
      "Epoch 2/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.7114 - decoder_loss: 0.5495 - energy_loss: 0.1619 - val_loss: 0.6662 - val_decoder_loss: 0.5154 - val_energy_loss: 0.1508\n",
      "Epoch 3/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.6387 - decoder_loss: 0.4889 - energy_loss: 0.1498 - val_loss: 0.6049 - val_decoder_loss: 0.4606 - val_energy_loss: 0.1442\n",
      "Epoch 4/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.5807 - decoder_loss: 0.4364 - energy_loss: 0.1443 - val_loss: 0.5706 - val_decoder_loss: 0.4136 - val_energy_loss: 0.1570\n",
      "Epoch 5/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.5361 - decoder_loss: 0.3983 - energy_loss: 0.1377 - val_loss: 0.5247 - val_decoder_loss: 0.3847 - val_energy_loss: 0.1400\n",
      "Epoch 6/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.5086 - decoder_loss: 0.3762 - energy_loss: 0.1324 - val_loss: 0.5314 - val_decoder_loss: 0.3665 - val_energy_loss: 0.1649\n",
      "Epoch 7/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4864 - decoder_loss: 0.3573 - energy_loss: 0.1291 - val_loss: 0.4801 - val_decoder_loss: 0.3453 - val_energy_loss: 0.1347\n",
      "Epoch 8/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4660 - decoder_loss: 0.3408 - energy_loss: 0.1252 - val_loss: 0.4680 - val_decoder_loss: 0.3326 - val_energy_loss: 0.1353\n",
      "Epoch 9/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4498 - decoder_loss: 0.3287 - energy_loss: 0.1211 - val_loss: 0.4545 - val_decoder_loss: 0.3226 - val_energy_loss: 0.1318\n",
      "Epoch 10/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4350 - decoder_loss: 0.3181 - energy_loss: 0.1168 - val_loss: 0.4406 - val_decoder_loss: 0.3110 - val_energy_loss: 0.1296\n",
      "Epoch 11/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4215 - decoder_loss: 0.3083 - energy_loss: 0.1132 - val_loss: 0.4576 - val_decoder_loss: 0.3050 - val_energy_loss: 0.1526\n",
      "Epoch 12/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4120 - decoder_loss: 0.3006 - energy_loss: 0.1114 - val_loss: 0.4240 - val_decoder_loss: 0.2954 - val_energy_loss: 0.1286\n",
      "Epoch 13/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.4019 - decoder_loss: 0.2928 - energy_loss: 0.1091 - val_loss: 0.4206 - val_decoder_loss: 0.2910 - val_energy_loss: 0.1295\n",
      "Epoch 14/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3936 - decoder_loss: 0.2862 - energy_loss: 0.1075 - val_loss: 0.4076 - val_decoder_loss: 0.2850 - val_energy_loss: 0.1226\n",
      "Epoch 15/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3883 - decoder_loss: 0.2818 - energy_loss: 0.1065 - val_loss: 0.4095 - val_decoder_loss: 0.2796 - val_energy_loss: 0.1298\n",
      "Epoch 16/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3819 - decoder_loss: 0.2785 - energy_loss: 0.1034 - val_loss: 0.4055 - val_decoder_loss: 0.2771 - val_energy_loss: 0.1283\n",
      "Epoch 17/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3776 - decoder_loss: 0.2753 - energy_loss: 0.1023 - val_loss: 0.4034 - val_decoder_loss: 0.2735 - val_energy_loss: 0.1299\n",
      "Epoch 18/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3711 - decoder_loss: 0.2710 - energy_loss: 0.1001 - val_loss: 0.3927 - val_decoder_loss: 0.2675 - val_energy_loss: 0.1251\n",
      "Epoch 19/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3669 - decoder_loss: 0.2656 - energy_loss: 0.1013 - val_loss: 0.4024 - val_decoder_loss: 0.2651 - val_energy_loss: 0.1373\n",
      "Epoch 20/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3618 - decoder_loss: 0.2620 - energy_loss: 0.0999 - val_loss: 0.3812 - val_decoder_loss: 0.2620 - val_energy_loss: 0.1192\n",
      "Epoch 21/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3568 - decoder_loss: 0.2597 - energy_loss: 0.0971 - val_loss: 0.3854 - val_decoder_loss: 0.2612 - val_energy_loss: 0.1242\n",
      "Epoch 22/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3556 - decoder_loss: 0.2586 - energy_loss: 0.0970 - val_loss: 0.3820 - val_decoder_loss: 0.2574 - val_energy_loss: 0.1246\n",
      "Epoch 23/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3537 - decoder_loss: 0.2580 - energy_loss: 0.0957 - val_loss: 0.3790 - val_decoder_loss: 0.2567 - val_energy_loss: 0.1224\n",
      "Epoch 24/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3511 - decoder_loss: 0.2571 - energy_loss: 0.0940 - val_loss: 0.3944 - val_decoder_loss: 0.2590 - val_energy_loss: 0.1354\n",
      "Epoch 25/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3516 - decoder_loss: 0.2564 - energy_loss: 0.0952 - val_loss: 0.3729 - val_decoder_loss: 0.2561 - val_energy_loss: 0.1168\n",
      "Epoch 26/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3484 - decoder_loss: 0.2558 - energy_loss: 0.0927 - val_loss: 0.3864 - val_decoder_loss: 0.2543 - val_energy_loss: 0.1321\n",
      "Epoch 27/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3478 - decoder_loss: 0.2553 - energy_loss: 0.0925 - val_loss: 0.3967 - val_decoder_loss: 0.2566 - val_energy_loss: 0.1401\n",
      "Epoch 28/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3450 - decoder_loss: 0.2544 - energy_loss: 0.0906 - val_loss: 0.3797 - val_decoder_loss: 0.2549 - val_energy_loss: 0.1248\n",
      "Epoch 29/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3432 - decoder_loss: 0.2533 - energy_loss: 0.0898 - val_loss: 0.3763 - val_decoder_loss: 0.2523 - val_energy_loss: 0.1240\n",
      "Epoch 30/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3408 - decoder_loss: 0.2510 - energy_loss: 0.0898 - val_loss: 0.3759 - val_decoder_loss: 0.2507 - val_energy_loss: 0.1252\n",
      "Epoch 31/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3335 - decoder_loss: 0.2458 - energy_loss: 0.0878 - val_loss: 0.3575 - val_decoder_loss: 0.2414 - val_energy_loss: 0.1162\n",
      "Epoch 32/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3287 - decoder_loss: 0.2401 - energy_loss: 0.0885 - val_loss: 0.3752 - val_decoder_loss: 0.2385 - val_energy_loss: 0.1366\n",
      "Epoch 33/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3245 - decoder_loss: 0.2370 - energy_loss: 0.0876 - val_loss: 0.3622 - val_decoder_loss: 0.2350 - val_energy_loss: 0.1272\n",
      "Epoch 34/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3216 - decoder_loss: 0.2350 - energy_loss: 0.0867 - val_loss: 0.3773 - val_decoder_loss: 0.2356 - val_energy_loss: 0.1416\n",
      "Epoch 35/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3199 - decoder_loss: 0.2333 - energy_loss: 0.0866 - val_loss: 0.3638 - val_decoder_loss: 0.2310 - val_energy_loss: 0.1328\n",
      "Epoch 36/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3175 - decoder_loss: 0.2314 - energy_loss: 0.0861 - val_loss: 0.3438 - val_decoder_loss: 0.2311 - val_energy_loss: 0.1127\n",
      "Epoch 37/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3136 - decoder_loss: 0.2287 - energy_loss: 0.0849 - val_loss: 0.3471 - val_decoder_loss: 0.2269 - val_energy_loss: 0.1202\n",
      "Epoch 38/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3112 - decoder_loss: 0.2266 - energy_loss: 0.0847 - val_loss: 0.3516 - val_decoder_loss: 0.2258 - val_energy_loss: 0.1258\n",
      "Epoch 39/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3091 - decoder_loss: 0.2249 - energy_loss: 0.0842 - val_loss: 0.3564 - val_decoder_loss: 0.2231 - val_energy_loss: 0.1333\n",
      "Epoch 40/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3076 - decoder_loss: 0.2234 - energy_loss: 0.0842 - val_loss: 0.3421 - val_decoder_loss: 0.2233 - val_energy_loss: 0.1188\n",
      "Epoch 41/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3055 - decoder_loss: 0.2229 - energy_loss: 0.0826 - val_loss: 0.3414 - val_decoder_loss: 0.2217 - val_energy_loss: 0.1197\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3055 - decoder_loss: 0.2219 - energy_loss: 0.0836 - val_loss: 0.3435 - val_decoder_loss: 0.2195 - val_energy_loss: 0.1240\n",
      "Epoch 43/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3044 - decoder_loss: 0.2213 - energy_loss: 0.0831 - val_loss: 0.3430 - val_decoder_loss: 0.2206 - val_energy_loss: 0.1224\n",
      "Epoch 44/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3025 - decoder_loss: 0.2208 - energy_loss: 0.0818 - val_loss: 0.3328 - val_decoder_loss: 0.2203 - val_energy_loss: 0.1125\n",
      "Epoch 45/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3036 - decoder_loss: 0.2203 - energy_loss: 0.0832 - val_loss: 0.3445 - val_decoder_loss: 0.2210 - val_energy_loss: 0.1235\n",
      "Epoch 46/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3019 - decoder_loss: 0.2203 - energy_loss: 0.0816 - val_loss: 0.3388 - val_decoder_loss: 0.2192 - val_energy_loss: 0.1196\n",
      "Epoch 47/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.3010 - decoder_loss: 0.2198 - energy_loss: 0.0812 - val_loss: 0.3289 - val_decoder_loss: 0.2184 - val_energy_loss: 0.1105\n",
      "Epoch 48/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2989 - decoder_loss: 0.2194 - energy_loss: 0.0795 - val_loss: 0.3363 - val_decoder_loss: 0.2190 - val_energy_loss: 0.1173\n",
      "Epoch 49/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2991 - decoder_loss: 0.2193 - energy_loss: 0.0798 - val_loss: 0.3309 - val_decoder_loss: 0.2197 - val_energy_loss: 0.1113\n",
      "Epoch 50/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2986 - decoder_loss: 0.2191 - energy_loss: 0.0795 - val_loss: 0.3316 - val_decoder_loss: 0.2182 - val_energy_loss: 0.1134\n",
      "Epoch 51/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2981 - decoder_loss: 0.2188 - energy_loss: 0.0794 - val_loss: 0.3361 - val_decoder_loss: 0.2204 - val_energy_loss: 0.1157\n",
      "Epoch 52/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2977 - decoder_loss: 0.2189 - energy_loss: 0.0788 - val_loss: 0.3291 - val_decoder_loss: 0.2193 - val_energy_loss: 0.1099\n",
      "Epoch 53/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2975 - decoder_loss: 0.2185 - energy_loss: 0.0790 - val_loss: 0.3552 - val_decoder_loss: 0.2189 - val_energy_loss: 0.1363\n",
      "Epoch 54/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2964 - decoder_loss: 0.2184 - energy_loss: 0.0779 - val_loss: 0.3351 - val_decoder_loss: 0.2174 - val_energy_loss: 0.1176\n",
      "Epoch 55/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2958 - decoder_loss: 0.2183 - energy_loss: 0.0775 - val_loss: 0.3501 - val_decoder_loss: 0.2195 - val_energy_loss: 0.1305\n",
      "Epoch 56/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2950 - decoder_loss: 0.2182 - energy_loss: 0.0768 - val_loss: 0.3446 - val_decoder_loss: 0.2169 - val_energy_loss: 0.1277\n",
      "Epoch 57/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2952 - decoder_loss: 0.2181 - energy_loss: 0.0772 - val_loss: 0.3434 - val_decoder_loss: 0.2179 - val_energy_loss: 0.1255\n",
      "Epoch 58/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2957 - decoder_loss: 0.2182 - energy_loss: 0.0775 - val_loss: 0.3369 - val_decoder_loss: 0.2186 - val_energy_loss: 0.1183\n",
      "Epoch 59/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2943 - decoder_loss: 0.2181 - energy_loss: 0.0763 - val_loss: 0.3342 - val_decoder_loss: 0.2171 - val_energy_loss: 0.1170\n",
      "Epoch 60/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2937 - decoder_loss: 0.2177 - energy_loss: 0.0761 - val_loss: 0.3359 - val_decoder_loss: 0.2176 - val_energy_loss: 0.1183\n",
      "Epoch 61/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2934 - decoder_loss: 0.2178 - energy_loss: 0.0756 - val_loss: 0.3293 - val_decoder_loss: 0.2172 - val_energy_loss: 0.1121\n",
      "Epoch 62/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2939 - decoder_loss: 0.2175 - energy_loss: 0.0764 - val_loss: 0.3271 - val_decoder_loss: 0.2171 - val_energy_loss: 0.1100\n",
      "Epoch 63/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2933 - decoder_loss: 0.2176 - energy_loss: 0.0757 - val_loss: 0.3381 - val_decoder_loss: 0.2176 - val_energy_loss: 0.1205\n",
      "Epoch 64/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2926 - decoder_loss: 0.2178 - energy_loss: 0.0748 - val_loss: 0.3447 - val_decoder_loss: 0.2176 - val_energy_loss: 0.1271\n",
      "Epoch 65/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2917 - decoder_loss: 0.2176 - energy_loss: 0.0742 - val_loss: 0.3287 - val_decoder_loss: 0.2178 - val_energy_loss: 0.1109\n",
      "Epoch 66/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2919 - decoder_loss: 0.2173 - energy_loss: 0.0746 - val_loss: 0.3350 - val_decoder_loss: 0.2173 - val_energy_loss: 0.1176\n",
      "Epoch 67/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2917 - decoder_loss: 0.2174 - energy_loss: 0.0743 - val_loss: 0.3302 - val_decoder_loss: 0.2172 - val_energy_loss: 0.1129\n",
      "Epoch 68/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2911 - decoder_loss: 0.2170 - energy_loss: 0.0742 - val_loss: 0.3319 - val_decoder_loss: 0.2168 - val_energy_loss: 0.1151\n",
      "Epoch 69/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2899 - decoder_loss: 0.2170 - energy_loss: 0.0729 - val_loss: 0.3380 - val_decoder_loss: 0.2180 - val_energy_loss: 0.1200\n",
      "Epoch 70/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2905 - decoder_loss: 0.2172 - energy_loss: 0.0733 - val_loss: 0.3312 - val_decoder_loss: 0.2166 - val_energy_loss: 0.1146\n",
      "Epoch 71/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2893 - decoder_loss: 0.2169 - energy_loss: 0.0724 - val_loss: 0.3255 - val_decoder_loss: 0.2168 - val_energy_loss: 0.1088\n",
      "Epoch 72/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2906 - decoder_loss: 0.2172 - energy_loss: 0.0735 - val_loss: 0.3386 - val_decoder_loss: 0.2175 - val_energy_loss: 0.1211\n",
      "Epoch 73/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2904 - decoder_loss: 0.2172 - energy_loss: 0.0732 - val_loss: 0.3424 - val_decoder_loss: 0.2167 - val_energy_loss: 0.1257\n",
      "Epoch 74/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2885 - decoder_loss: 0.2168 - energy_loss: 0.0717 - val_loss: 0.3463 - val_decoder_loss: 0.2179 - val_energy_loss: 0.1284\n",
      "Epoch 75/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2881 - decoder_loss: 0.2168 - energy_loss: 0.0713 - val_loss: 0.3491 - val_decoder_loss: 0.2196 - val_energy_loss: 0.1295\n",
      "Epoch 76/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2894 - decoder_loss: 0.2169 - energy_loss: 0.0724 - val_loss: 0.3256 - val_decoder_loss: 0.2168 - val_energy_loss: 0.1088\n",
      "Epoch 77/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2885 - decoder_loss: 0.2167 - energy_loss: 0.0718 - val_loss: 0.3475 - val_decoder_loss: 0.2160 - val_energy_loss: 0.1314\n",
      "Epoch 78/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2890 - decoder_loss: 0.2166 - energy_loss: 0.0725 - val_loss: 0.3310 - val_decoder_loss: 0.2173 - val_energy_loss: 0.1137\n",
      "Epoch 79/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2870 - decoder_loss: 0.2165 - energy_loss: 0.0706 - val_loss: 0.3298 - val_decoder_loss: 0.2169 - val_energy_loss: 0.1129\n",
      "Epoch 80/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2876 - decoder_loss: 0.2167 - energy_loss: 0.0709 - val_loss: 0.3397 - val_decoder_loss: 0.2192 - val_energy_loss: 0.1205\n",
      "Epoch 81/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2867 - decoder_loss: 0.2162 - energy_loss: 0.0705 - val_loss: 0.3319 - val_decoder_loss: 0.2169 - val_energy_loss: 0.1150\n",
      "Epoch 82/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2866 - decoder_loss: 0.2159 - energy_loss: 0.0707 - val_loss: 0.3301 - val_decoder_loss: 0.2162 - val_energy_loss: 0.1140\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2861 - decoder_loss: 0.2155 - energy_loss: 0.0706 - val_loss: 0.3425 - val_decoder_loss: 0.2153 - val_energy_loss: 0.1273\n",
      "Epoch 84/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2844 - decoder_loss: 0.2140 - energy_loss: 0.0704 - val_loss: 0.3293 - val_decoder_loss: 0.2137 - val_energy_loss: 0.1156\n",
      "Epoch 85/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2806 - decoder_loss: 0.2112 - energy_loss: 0.0694 - val_loss: 0.3247 - val_decoder_loss: 0.2085 - val_energy_loss: 0.1162\n",
      "Epoch 86/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2776 - decoder_loss: 0.2071 - energy_loss: 0.0705 - val_loss: 0.3221 - val_decoder_loss: 0.2059 - val_energy_loss: 0.1162\n",
      "Epoch 87/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2743 - decoder_loss: 0.2041 - energy_loss: 0.0702 - val_loss: 0.3198 - val_decoder_loss: 0.2041 - val_energy_loss: 0.1156\n",
      "Epoch 88/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2729 - decoder_loss: 0.2026 - energy_loss: 0.0703 - val_loss: 0.3171 - val_decoder_loss: 0.2013 - val_energy_loss: 0.1158\n",
      "Epoch 89/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2718 - decoder_loss: 0.2018 - energy_loss: 0.0700 - val_loss: 0.3394 - val_decoder_loss: 0.2017 - val_energy_loss: 0.1377\n",
      "Epoch 90/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2717 - decoder_loss: 0.2014 - energy_loss: 0.0703 - val_loss: 0.3231 - val_decoder_loss: 0.2011 - val_energy_loss: 0.1221\n",
      "Epoch 91/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2702 - decoder_loss: 0.2008 - energy_loss: 0.0694 - val_loss: 0.3124 - val_decoder_loss: 0.2001 - val_energy_loss: 0.1123\n",
      "Epoch 92/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2695 - decoder_loss: 0.2008 - energy_loss: 0.0687 - val_loss: 0.3135 - val_decoder_loss: 0.1997 - val_energy_loss: 0.1138\n",
      "Epoch 93/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2689 - decoder_loss: 0.2003 - energy_loss: 0.0686 - val_loss: 0.3241 - val_decoder_loss: 0.1997 - val_energy_loss: 0.1243\n",
      "Epoch 94/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2687 - decoder_loss: 0.2002 - energy_loss: 0.0685 - val_loss: 0.3229 - val_decoder_loss: 0.2008 - val_energy_loss: 0.1222\n",
      "Epoch 95/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2685 - decoder_loss: 0.2001 - energy_loss: 0.0684 - val_loss: 0.3167 - val_decoder_loss: 0.2001 - val_energy_loss: 0.1166\n",
      "Epoch 96/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2682 - decoder_loss: 0.1999 - energy_loss: 0.0682 - val_loss: 0.3154 - val_decoder_loss: 0.1996 - val_energy_loss: 0.1158\n",
      "Epoch 97/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2679 - decoder_loss: 0.1998 - energy_loss: 0.0681 - val_loss: 0.3157 - val_decoder_loss: 0.1992 - val_energy_loss: 0.1165\n",
      "Epoch 98/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2666 - decoder_loss: 0.1995 - energy_loss: 0.0670 - val_loss: 0.3102 - val_decoder_loss: 0.1994 - val_energy_loss: 0.1108\n",
      "Epoch 99/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2679 - decoder_loss: 0.1995 - energy_loss: 0.0684 - val_loss: 0.3097 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1115\n",
      "Epoch 100/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2666 - decoder_loss: 0.1996 - energy_loss: 0.0671 - val_loss: 0.3229 - val_decoder_loss: 0.1990 - val_energy_loss: 0.1238\n",
      "Epoch 101/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2668 - decoder_loss: 0.1994 - energy_loss: 0.0674 - val_loss: 0.3107 - val_decoder_loss: 0.1997 - val_energy_loss: 0.1110\n",
      "Epoch 102/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2669 - decoder_loss: 0.1995 - energy_loss: 0.0674 - val_loss: 0.3211 - val_decoder_loss: 0.1989 - val_energy_loss: 0.1221\n",
      "Epoch 103/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2668 - decoder_loss: 0.1994 - energy_loss: 0.0675 - val_loss: 0.3116 - val_decoder_loss: 0.1985 - val_energy_loss: 0.1131\n",
      "Epoch 104/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2659 - decoder_loss: 0.1992 - energy_loss: 0.0667 - val_loss: 0.3069 - val_decoder_loss: 0.1988 - val_energy_loss: 0.1081\n",
      "Epoch 105/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2666 - decoder_loss: 0.1992 - energy_loss: 0.0673 - val_loss: 0.3217 - val_decoder_loss: 0.1998 - val_energy_loss: 0.1219\n",
      "Epoch 106/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2660 - decoder_loss: 0.1993 - energy_loss: 0.0667 - val_loss: 0.3207 - val_decoder_loss: 0.1997 - val_energy_loss: 0.1210\n",
      "Epoch 107/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2657 - decoder_loss: 0.1992 - energy_loss: 0.0665 - val_loss: 0.3193 - val_decoder_loss: 0.1995 - val_energy_loss: 0.1198\n",
      "Epoch 108/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2661 - decoder_loss: 0.1993 - energy_loss: 0.0668 - val_loss: 0.3102 - val_decoder_loss: 0.1993 - val_energy_loss: 0.1109\n",
      "Epoch 109/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2646 - decoder_loss: 0.1990 - energy_loss: 0.0656 - val_loss: 0.3096 - val_decoder_loss: 0.1994 - val_energy_loss: 0.1103\n",
      "Epoch 110/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2660 - decoder_loss: 0.1991 - energy_loss: 0.0669 - val_loss: 0.3223 - val_decoder_loss: 0.1991 - val_energy_loss: 0.1232\n",
      "Epoch 111/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2650 - decoder_loss: 0.1991 - energy_loss: 0.0659 - val_loss: 0.3111 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1127\n",
      "Epoch 112/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2651 - decoder_loss: 0.1989 - energy_loss: 0.0662 - val_loss: 0.3127 - val_decoder_loss: 0.1987 - val_energy_loss: 0.1141\n",
      "Epoch 113/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2645 - decoder_loss: 0.1989 - energy_loss: 0.0656 - val_loss: 0.3163 - val_decoder_loss: 0.1986 - val_energy_loss: 0.1177\n",
      "Epoch 114/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2649 - decoder_loss: 0.1989 - energy_loss: 0.0660 - val_loss: 0.3141 - val_decoder_loss: 0.1985 - val_energy_loss: 0.1156\n",
      "Epoch 115/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2646 - decoder_loss: 0.1988 - energy_loss: 0.0658 - val_loss: 0.3173 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1191\n",
      "Epoch 116/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2643 - decoder_loss: 0.1987 - energy_loss: 0.0657 - val_loss: 0.3221 - val_decoder_loss: 0.1997 - val_energy_loss: 0.1224\n",
      "Epoch 117/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2648 - decoder_loss: 0.1988 - energy_loss: 0.0660 - val_loss: 0.3194 - val_decoder_loss: 0.1994 - val_energy_loss: 0.1201\n",
      "Epoch 118/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2641 - decoder_loss: 0.1986 - energy_loss: 0.0654 - val_loss: 0.3164 - val_decoder_loss: 0.1987 - val_energy_loss: 0.1177\n",
      "Epoch 119/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2632 - decoder_loss: 0.1987 - energy_loss: 0.0645 - val_loss: 0.3170 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1186\n",
      "Epoch 120/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2647 - decoder_loss: 0.1990 - energy_loss: 0.0657 - val_loss: 0.3178 - val_decoder_loss: 0.1990 - val_energy_loss: 0.1188\n",
      "Epoch 121/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2641 - decoder_loss: 0.1988 - energy_loss: 0.0653 - val_loss: 0.3154 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1172\n",
      "Epoch 122/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2649 - decoder_loss: 0.1987 - energy_loss: 0.0662 - val_loss: 0.3173 - val_decoder_loss: 0.1988 - val_energy_loss: 0.1184\n",
      "Epoch 123/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2631 - decoder_loss: 0.1987 - energy_loss: 0.0644 - val_loss: 0.3238 - val_decoder_loss: 0.1986 - val_energy_loss: 0.1253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2633 - decoder_loss: 0.1986 - energy_loss: 0.0647 - val_loss: 0.3187 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1203\n",
      "Epoch 125/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2626 - decoder_loss: 0.1985 - energy_loss: 0.0641 - val_loss: 0.3098 - val_decoder_loss: 0.1986 - val_energy_loss: 0.1111\n",
      "Epoch 126/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2633 - decoder_loss: 0.1987 - energy_loss: 0.0646 - val_loss: 0.3117 - val_decoder_loss: 0.1992 - val_energy_loss: 0.1126\n",
      "Epoch 127/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2632 - decoder_loss: 0.1986 - energy_loss: 0.0646 - val_loss: 0.3116 - val_decoder_loss: 0.1976 - val_energy_loss: 0.1140\n",
      "Epoch 128/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2633 - decoder_loss: 0.1985 - energy_loss: 0.0648 - val_loss: 0.3178 - val_decoder_loss: 0.1987 - val_energy_loss: 0.1191\n",
      "Epoch 129/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2630 - decoder_loss: 0.1985 - energy_loss: 0.0644 - val_loss: 0.3126 - val_decoder_loss: 0.2014 - val_energy_loss: 0.1112\n",
      "Epoch 130/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2628 - decoder_loss: 0.1987 - energy_loss: 0.0641 - val_loss: 0.3190 - val_decoder_loss: 0.1987 - val_energy_loss: 0.1203\n",
      "Epoch 131/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2618 - decoder_loss: 0.1984 - energy_loss: 0.0633 - val_loss: 0.3170 - val_decoder_loss: 0.1983 - val_energy_loss: 0.1187\n",
      "Epoch 132/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2630 - decoder_loss: 0.1985 - energy_loss: 0.0645 - val_loss: 0.3231 - val_decoder_loss: 0.1980 - val_energy_loss: 0.1251\n",
      "Epoch 133/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2619 - decoder_loss: 0.1984 - energy_loss: 0.0635 - val_loss: 0.3186 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1202\n",
      "Epoch 134/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2623 - decoder_loss: 0.1985 - energy_loss: 0.0638 - val_loss: 0.3107 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1123\n",
      "Epoch 135/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2613 - decoder_loss: 0.1983 - energy_loss: 0.0630 - val_loss: 0.3089 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1105\n",
      "Epoch 136/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2625 - decoder_loss: 0.1984 - energy_loss: 0.0642 - val_loss: 0.3117 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1133\n",
      "Epoch 137/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2617 - decoder_loss: 0.1982 - energy_loss: 0.0636 - val_loss: 0.3051 - val_decoder_loss: 0.1988 - val_energy_loss: 0.1063\n",
      "Epoch 138/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2619 - decoder_loss: 0.1983 - energy_loss: 0.0636 - val_loss: 0.3193 - val_decoder_loss: 0.1975 - val_energy_loss: 0.1218\n",
      "Epoch 139/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2621 - decoder_loss: 0.1985 - energy_loss: 0.0636 - val_loss: 0.3120 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1135\n",
      "Epoch 140/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2622 - decoder_loss: 0.1986 - energy_loss: 0.0636 - val_loss: 0.3184 - val_decoder_loss: 0.1990 - val_energy_loss: 0.1194\n",
      "Epoch 141/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2616 - decoder_loss: 0.1985 - energy_loss: 0.0631 - val_loss: 0.3133 - val_decoder_loss: 0.1998 - val_energy_loss: 0.1135\n",
      "Epoch 142/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2610 - decoder_loss: 0.1983 - energy_loss: 0.0627 - val_loss: 0.3220 - val_decoder_loss: 0.1997 - val_energy_loss: 0.1223\n",
      "Epoch 143/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2613 - decoder_loss: 0.1983 - energy_loss: 0.0630 - val_loss: 0.3128 - val_decoder_loss: 0.1992 - val_energy_loss: 0.1135\n",
      "Epoch 144/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2603 - decoder_loss: 0.1982 - energy_loss: 0.0620 - val_loss: 0.3109 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1127\n",
      "Epoch 145/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2618 - decoder_loss: 0.1985 - energy_loss: 0.0633 - val_loss: 0.3108 - val_decoder_loss: 0.1985 - val_energy_loss: 0.1123\n",
      "Epoch 146/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2611 - decoder_loss: 0.1982 - energy_loss: 0.0629 - val_loss: 0.3144 - val_decoder_loss: 0.1985 - val_energy_loss: 0.1158\n",
      "Epoch 147/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2611 - decoder_loss: 0.1984 - energy_loss: 0.0627 - val_loss: 0.3233 - val_decoder_loss: 0.1986 - val_energy_loss: 0.1247\n",
      "Epoch 148/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2607 - decoder_loss: 0.1982 - energy_loss: 0.0625 - val_loss: 0.3223 - val_decoder_loss: 0.1980 - val_energy_loss: 0.1243\n",
      "Epoch 149/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2598 - decoder_loss: 0.1982 - energy_loss: 0.0616 - val_loss: 0.3236 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1254\n",
      "Epoch 150/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2608 - decoder_loss: 0.1982 - energy_loss: 0.0626 - val_loss: 0.3174 - val_decoder_loss: 0.1988 - val_energy_loss: 0.1186\n",
      "Epoch 151/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2603 - decoder_loss: 0.1983 - energy_loss: 0.0619 - val_loss: 0.3158 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1173\n",
      "Epoch 152/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2610 - decoder_loss: 0.1983 - energy_loss: 0.0627 - val_loss: 0.3229 - val_decoder_loss: 0.1993 - val_energy_loss: 0.1236\n",
      "Epoch 153/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2610 - decoder_loss: 0.1985 - energy_loss: 0.0625 - val_loss: 0.3155 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1174\n",
      "Epoch 154/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2599 - decoder_loss: 0.1981 - energy_loss: 0.0618 - val_loss: 0.3196 - val_decoder_loss: 0.1988 - val_energy_loss: 0.1208\n",
      "Epoch 155/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2603 - decoder_loss: 0.1982 - energy_loss: 0.0621 - val_loss: 0.3143 - val_decoder_loss: 0.1987 - val_energy_loss: 0.1156\n",
      "Epoch 156/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2596 - decoder_loss: 0.1981 - energy_loss: 0.0615 - val_loss: 0.3101 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1117\n",
      "Epoch 157/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2591 - decoder_loss: 0.1981 - energy_loss: 0.0609 - val_loss: 0.3193 - val_decoder_loss: 0.1988 - val_energy_loss: 0.1205\n",
      "Epoch 158/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2598 - decoder_loss: 0.1981 - energy_loss: 0.0617 - val_loss: 0.3173 - val_decoder_loss: 0.1980 - val_energy_loss: 0.1193\n",
      "Epoch 159/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2603 - decoder_loss: 0.1982 - energy_loss: 0.0621 - val_loss: 0.3140 - val_decoder_loss: 0.1978 - val_energy_loss: 0.1162\n",
      "Epoch 160/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2599 - decoder_loss: 0.1984 - energy_loss: 0.0615 - val_loss: 0.3231 - val_decoder_loss: 0.1990 - val_energy_loss: 0.1241\n",
      "Epoch 161/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2597 - decoder_loss: 0.1982 - energy_loss: 0.0615 - val_loss: 0.3152 - val_decoder_loss: 0.2004 - val_energy_loss: 0.1148\n",
      "Epoch 162/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2588 - decoder_loss: 0.1982 - energy_loss: 0.0606 - val_loss: 0.3229 - val_decoder_loss: 0.1981 - val_energy_loss: 0.1249\n",
      "Epoch 163/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2583 - decoder_loss: 0.1978 - energy_loss: 0.0606 - val_loss: 0.3074 - val_decoder_loss: 0.1985 - val_energy_loss: 0.1089\n",
      "Epoch 164/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2595 - decoder_loss: 0.1981 - energy_loss: 0.0615 - val_loss: 0.3304 - val_decoder_loss: 0.1975 - val_energy_loss: 0.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2587 - decoder_loss: 0.1981 - energy_loss: 0.0606 - val_loss: 0.3138 - val_decoder_loss: 0.1983 - val_energy_loss: 0.1155\n",
      "Epoch 166/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2590 - decoder_loss: 0.1980 - energy_loss: 0.0610 - val_loss: 0.3148 - val_decoder_loss: 0.1985 - val_energy_loss: 0.1163\n",
      "Epoch 167/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2589 - decoder_loss: 0.1981 - energy_loss: 0.0608 - val_loss: 0.3308 - val_decoder_loss: 0.1981 - val_energy_loss: 0.1327\n",
      "Epoch 168/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2580 - decoder_loss: 0.1978 - energy_loss: 0.0602 - val_loss: 0.3174 - val_decoder_loss: 0.1983 - val_energy_loss: 0.1192\n",
      "Epoch 169/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2586 - decoder_loss: 0.1981 - energy_loss: 0.0605 - val_loss: 0.3168 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1184\n",
      "Epoch 170/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2595 - decoder_loss: 0.1984 - energy_loss: 0.0611 - val_loss: 0.3333 - val_decoder_loss: 0.1987 - val_energy_loss: 0.1346\n",
      "Epoch 171/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2584 - decoder_loss: 0.1979 - energy_loss: 0.0605 - val_loss: 0.3119 - val_decoder_loss: 0.1992 - val_energy_loss: 0.1127\n",
      "Epoch 172/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2577 - decoder_loss: 0.1978 - energy_loss: 0.0599 - val_loss: 0.3165 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1180\n",
      "Epoch 173/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2586 - decoder_loss: 0.1981 - energy_loss: 0.0605 - val_loss: 0.3215 - val_decoder_loss: 0.1999 - val_energy_loss: 0.1217\n",
      "Epoch 174/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2583 - decoder_loss: 0.1980 - energy_loss: 0.0603 - val_loss: 0.3110 - val_decoder_loss: 0.1979 - val_energy_loss: 0.1131\n",
      "Epoch 175/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2585 - decoder_loss: 0.1980 - energy_loss: 0.0605 - val_loss: 0.3182 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1198\n",
      "Epoch 176/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2579 - decoder_loss: 0.1981 - energy_loss: 0.0598 - val_loss: 0.3087 - val_decoder_loss: 0.1986 - val_energy_loss: 0.1102\n",
      "Epoch 177/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2580 - decoder_loss: 0.1980 - energy_loss: 0.0599 - val_loss: 0.3139 - val_decoder_loss: 0.1976 - val_energy_loss: 0.1163\n",
      "Epoch 178/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2573 - decoder_loss: 0.1980 - energy_loss: 0.0593 - val_loss: 0.3141 - val_decoder_loss: 0.1981 - val_energy_loss: 0.1160\n",
      "Epoch 179/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2578 - decoder_loss: 0.1979 - energy_loss: 0.0598 - val_loss: 0.3197 - val_decoder_loss: 0.1991 - val_energy_loss: 0.1206\n",
      "Epoch 180/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2572 - decoder_loss: 0.1980 - energy_loss: 0.0593 - val_loss: 0.3123 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1141\n",
      "Epoch 181/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2586 - decoder_loss: 0.1981 - energy_loss: 0.0604 - val_loss: 0.3118 - val_decoder_loss: 0.1981 - val_energy_loss: 0.1138\n",
      "Epoch 182/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2571 - decoder_loss: 0.1977 - energy_loss: 0.0593 - val_loss: 0.3157 - val_decoder_loss: 0.1977 - val_energy_loss: 0.1180\n",
      "Epoch 183/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2580 - decoder_loss: 0.1978 - energy_loss: 0.0602 - val_loss: 0.3106 - val_decoder_loss: 0.1996 - val_energy_loss: 0.1110\n",
      "Epoch 184/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2577 - decoder_loss: 0.1980 - energy_loss: 0.0597 - val_loss: 0.3152 - val_decoder_loss: 0.1994 - val_energy_loss: 0.1159\n",
      "Epoch 185/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2575 - decoder_loss: 0.1982 - energy_loss: 0.0593 - val_loss: 0.3124 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1143\n",
      "Epoch 186/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2573 - decoder_loss: 0.1978 - energy_loss: 0.0595 - val_loss: 0.3128 - val_decoder_loss: 0.1990 - val_energy_loss: 0.1138\n",
      "Epoch 187/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2583 - decoder_loss: 0.1981 - energy_loss: 0.0602 - val_loss: 0.3209 - val_decoder_loss: 0.1996 - val_energy_loss: 0.1213\n",
      "Epoch 188/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2573 - decoder_loss: 0.1978 - energy_loss: 0.0595 - val_loss: 0.3081 - val_decoder_loss: 0.1982 - val_energy_loss: 0.1099\n",
      "Epoch 189/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2576 - decoder_loss: 0.1980 - energy_loss: 0.0596 - val_loss: 0.3101 - val_decoder_loss: 0.1981 - val_energy_loss: 0.1120\n",
      "Epoch 190/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2570 - decoder_loss: 0.1978 - energy_loss: 0.0592 - val_loss: 0.3207 - val_decoder_loss: 0.1984 - val_energy_loss: 0.1223\n",
      "Epoch 191/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2575 - decoder_loss: 0.1980 - energy_loss: 0.0595 - val_loss: 0.3100 - val_decoder_loss: 0.1979 - val_energy_loss: 0.1121\n",
      "Epoch 192/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2571 - decoder_loss: 0.1979 - energy_loss: 0.0592 - val_loss: 0.3139 - val_decoder_loss: 0.1983 - val_energy_loss: 0.1156\n",
      "Epoch 193/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2569 - decoder_loss: 0.1978 - energy_loss: 0.0591 - val_loss: 0.3147 - val_decoder_loss: 0.1974 - val_energy_loss: 0.1173\n",
      "Epoch 194/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2558 - decoder_loss: 0.1976 - energy_loss: 0.0582 - val_loss: 0.3189 - val_decoder_loss: 0.1980 - val_energy_loss: 0.1209\n",
      "Epoch 195/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2572 - decoder_loss: 0.1985 - energy_loss: 0.0587 - val_loss: 0.3176 - val_decoder_loss: 0.1991 - val_energy_loss: 0.1186\n",
      "Epoch 196/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2570 - decoder_loss: 0.1977 - energy_loss: 0.0593 - val_loss: 0.3233 - val_decoder_loss: 0.1977 - val_energy_loss: 0.1257\n",
      "Epoch 197/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2560 - decoder_loss: 0.1976 - energy_loss: 0.0584 - val_loss: 0.3126 - val_decoder_loss: 0.1977 - val_energy_loss: 0.1149\n",
      "Epoch 198/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2562 - decoder_loss: 0.1972 - energy_loss: 0.0589 - val_loss: 0.3199 - val_decoder_loss: 0.1973 - val_energy_loss: 0.1225\n",
      "Epoch 199/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2553 - decoder_loss: 0.1964 - energy_loss: 0.0589 - val_loss: 0.3104 - val_decoder_loss: 0.1949 - val_energy_loss: 0.1154\n",
      "Epoch 200/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2518 - decoder_loss: 0.1934 - energy_loss: 0.0584 - val_loss: 0.3268 - val_decoder_loss: 0.1914 - val_energy_loss: 0.1355\n",
      "Epoch 201/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2486 - decoder_loss: 0.1894 - energy_loss: 0.0592 - val_loss: 0.3010 - val_decoder_loss: 0.1906 - val_energy_loss: 0.1103\n",
      "Epoch 202/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2446 - decoder_loss: 0.1860 - energy_loss: 0.0585 - val_loss: 0.2968 - val_decoder_loss: 0.1855 - val_energy_loss: 0.1114\n",
      "Epoch 203/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2430 - decoder_loss: 0.1844 - energy_loss: 0.0586 - val_loss: 0.2963 - val_decoder_loss: 0.1845 - val_energy_loss: 0.1117\n",
      "Epoch 204/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2426 - decoder_loss: 0.1839 - energy_loss: 0.0587 - val_loss: 0.3032 - val_decoder_loss: 0.1843 - val_energy_loss: 0.1189\n",
      "Epoch 205/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2412 - decoder_loss: 0.1829 - energy_loss: 0.0583 - val_loss: 0.3069 - val_decoder_loss: 0.1826 - val_energy_loss: 0.1243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2410 - decoder_loss: 0.1827 - energy_loss: 0.0583 - val_loss: 0.2980 - val_decoder_loss: 0.1834 - val_energy_loss: 0.1146\n",
      "Epoch 207/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2410 - decoder_loss: 0.1825 - energy_loss: 0.0584 - val_loss: 0.3001 - val_decoder_loss: 0.1825 - val_energy_loss: 0.1176\n",
      "Epoch 208/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2405 - decoder_loss: 0.1821 - energy_loss: 0.0584 - val_loss: 0.2996 - val_decoder_loss: 0.1826 - val_energy_loss: 0.1170\n",
      "Epoch 209/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2404 - decoder_loss: 0.1819 - energy_loss: 0.0585 - val_loss: 0.2930 - val_decoder_loss: 0.1811 - val_energy_loss: 0.1119\n",
      "Epoch 210/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2402 - decoder_loss: 0.1816 - energy_loss: 0.0586 - val_loss: 0.3031 - val_decoder_loss: 0.1817 - val_energy_loss: 0.1214\n",
      "Epoch 211/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2406 - decoder_loss: 0.1814 - energy_loss: 0.0591 - val_loss: 0.3057 - val_decoder_loss: 0.1814 - val_energy_loss: 0.1242\n",
      "Epoch 212/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2392 - decoder_loss: 0.1813 - energy_loss: 0.0579 - val_loss: 0.2986 - val_decoder_loss: 0.1818 - val_energy_loss: 0.1167\n",
      "Epoch 213/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2393 - decoder_loss: 0.1812 - energy_loss: 0.0582 - val_loss: 0.2982 - val_decoder_loss: 0.1816 - val_energy_loss: 0.1166\n",
      "Epoch 214/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2386 - decoder_loss: 0.1807 - energy_loss: 0.0579 - val_loss: 0.2975 - val_decoder_loss: 0.1809 - val_energy_loss: 0.1165\n",
      "Epoch 215/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2377 - decoder_loss: 0.1806 - energy_loss: 0.0572 - val_loss: 0.2951 - val_decoder_loss: 0.1796 - val_energy_loss: 0.1154\n",
      "Epoch 216/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2370 - decoder_loss: 0.1800 - energy_loss: 0.0570 - val_loss: 0.2981 - val_decoder_loss: 0.1812 - val_energy_loss: 0.1169\n",
      "Epoch 217/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2378 - decoder_loss: 0.1799 - energy_loss: 0.0580 - val_loss: 0.2966 - val_decoder_loss: 0.1794 - val_energy_loss: 0.1172\n",
      "Epoch 218/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2368 - decoder_loss: 0.1796 - energy_loss: 0.0573 - val_loss: 0.2960 - val_decoder_loss: 0.1794 - val_energy_loss: 0.1166\n",
      "Epoch 219/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2356 - decoder_loss: 0.1788 - energy_loss: 0.0569 - val_loss: 0.2944 - val_decoder_loss: 0.1789 - val_energy_loss: 0.1155\n",
      "Epoch 220/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2353 - decoder_loss: 0.1784 - energy_loss: 0.0569 - val_loss: 0.2973 - val_decoder_loss: 0.1788 - val_energy_loss: 0.1185\n",
      "Epoch 221/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2357 - decoder_loss: 0.1780 - energy_loss: 0.0577 - val_loss: 0.2889 - val_decoder_loss: 0.1785 - val_energy_loss: 0.1104\n",
      "Epoch 222/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2347 - decoder_loss: 0.1773 - energy_loss: 0.0574 - val_loss: 0.2927 - val_decoder_loss: 0.1775 - val_energy_loss: 0.1152\n",
      "Epoch 223/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2339 - decoder_loss: 0.1768 - energy_loss: 0.0571 - val_loss: 0.2955 - val_decoder_loss: 0.1764 - val_energy_loss: 0.1191\n",
      "Epoch 224/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2336 - decoder_loss: 0.1763 - energy_loss: 0.0573 - val_loss: 0.2873 - val_decoder_loss: 0.1763 - val_energy_loss: 0.1109\n",
      "Epoch 225/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2328 - decoder_loss: 0.1756 - energy_loss: 0.0572 - val_loss: 0.2852 - val_decoder_loss: 0.1756 - val_energy_loss: 0.1097\n",
      "Epoch 226/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2325 - decoder_loss: 0.1750 - energy_loss: 0.0575 - val_loss: 0.2962 - val_decoder_loss: 0.1743 - val_energy_loss: 0.1219\n",
      "Epoch 227/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2317 - decoder_loss: 0.1741 - energy_loss: 0.0576 - val_loss: 0.2930 - val_decoder_loss: 0.1740 - val_energy_loss: 0.1189\n",
      "Epoch 228/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2305 - decoder_loss: 0.1734 - energy_loss: 0.0570 - val_loss: 0.2900 - val_decoder_loss: 0.1722 - val_energy_loss: 0.1178\n",
      "Epoch 229/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2287 - decoder_loss: 0.1721 - energy_loss: 0.0566 - val_loss: 0.2870 - val_decoder_loss: 0.1708 - val_energy_loss: 0.1162\n",
      "Epoch 230/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2286 - decoder_loss: 0.1712 - energy_loss: 0.0574 - val_loss: 0.2880 - val_decoder_loss: 0.1713 - val_energy_loss: 0.1167\n",
      "Epoch 231/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2273 - decoder_loss: 0.1703 - energy_loss: 0.0570 - val_loss: 0.2820 - val_decoder_loss: 0.1687 - val_energy_loss: 0.1133\n",
      "Epoch 232/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2262 - decoder_loss: 0.1693 - energy_loss: 0.0569 - val_loss: 0.2852 - val_decoder_loss: 0.1682 - val_energy_loss: 0.1170\n",
      "Epoch 233/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2253 - decoder_loss: 0.1689 - energy_loss: 0.0564 - val_loss: 0.2952 - val_decoder_loss: 0.1681 - val_energy_loss: 0.1271\n",
      "Epoch 234/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2260 - decoder_loss: 0.1686 - energy_loss: 0.0573 - val_loss: 0.2904 - val_decoder_loss: 0.1694 - val_energy_loss: 0.1210\n",
      "Epoch 235/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2250 - decoder_loss: 0.1680 - energy_loss: 0.0570 - val_loss: 0.2847 - val_decoder_loss: 0.1666 - val_energy_loss: 0.1182\n",
      "Epoch 236/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2240 - decoder_loss: 0.1676 - energy_loss: 0.0564 - val_loss: 0.2859 - val_decoder_loss: 0.1668 - val_energy_loss: 0.1191\n",
      "Epoch 237/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2242 - decoder_loss: 0.1674 - energy_loss: 0.0567 - val_loss: 0.2839 - val_decoder_loss: 0.1664 - val_energy_loss: 0.1175\n",
      "Epoch 238/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2243 - decoder_loss: 0.1671 - energy_loss: 0.0572 - val_loss: 0.2801 - val_decoder_loss: 0.1655 - val_energy_loss: 0.1146\n",
      "Epoch 239/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2254 - decoder_loss: 0.1677 - energy_loss: 0.0577 - val_loss: 0.2853 - val_decoder_loss: 0.1660 - val_energy_loss: 0.1193\n",
      "Epoch 240/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2234 - decoder_loss: 0.1667 - energy_loss: 0.0567 - val_loss: 0.2811 - val_decoder_loss: 0.1655 - val_energy_loss: 0.1156\n",
      "Epoch 241/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2235 - decoder_loss: 0.1665 - energy_loss: 0.0570 - val_loss: 0.2814 - val_decoder_loss: 0.1662 - val_energy_loss: 0.1152\n",
      "Epoch 242/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2233 - decoder_loss: 0.1666 - energy_loss: 0.0567 - val_loss: 0.2793 - val_decoder_loss: 0.1656 - val_energy_loss: 0.1137\n",
      "Epoch 243/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2231 - decoder_loss: 0.1663 - energy_loss: 0.0568 - val_loss: 0.2935 - val_decoder_loss: 0.1667 - val_energy_loss: 0.1268\n",
      "Epoch 244/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2225 - decoder_loss: 0.1660 - energy_loss: 0.0565 - val_loss: 0.2958 - val_decoder_loss: 0.1654 - val_energy_loss: 0.1304\n",
      "Epoch 245/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2226 - decoder_loss: 0.1659 - energy_loss: 0.0566 - val_loss: 0.2782 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1139\n",
      "Epoch 246/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2234 - decoder_loss: 0.1662 - energy_loss: 0.0572 - val_loss: 0.2832 - val_decoder_loss: 0.1656 - val_energy_loss: 0.1176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2219 - decoder_loss: 0.1657 - energy_loss: 0.0562 - val_loss: 0.2912 - val_decoder_loss: 0.1662 - val_energy_loss: 0.1250\n",
      "Epoch 248/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2221 - decoder_loss: 0.1659 - energy_loss: 0.0562 - val_loss: 0.2753 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1110\n",
      "Epoch 249/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2217 - decoder_loss: 0.1658 - energy_loss: 0.0558 - val_loss: 0.2833 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1187\n",
      "Epoch 250/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2225 - decoder_loss: 0.1659 - energy_loss: 0.0567 - val_loss: 0.2872 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1229\n",
      "Epoch 251/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2225 - decoder_loss: 0.1665 - energy_loss: 0.0560 - val_loss: 0.2782 - val_decoder_loss: 0.1651 - val_energy_loss: 0.1130\n",
      "Epoch 252/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2218 - decoder_loss: 0.1655 - energy_loss: 0.0563 - val_loss: 0.2860 - val_decoder_loss: 0.1648 - val_energy_loss: 0.1213\n",
      "Epoch 253/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2215 - decoder_loss: 0.1652 - energy_loss: 0.0563 - val_loss: 0.2976 - val_decoder_loss: 0.1656 - val_energy_loss: 0.1321\n",
      "Epoch 254/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2217 - decoder_loss: 0.1656 - energy_loss: 0.0561 - val_loss: 0.2735 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1090\n",
      "Epoch 255/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2214 - decoder_loss: 0.1653 - energy_loss: 0.0561 - val_loss: 0.2950 - val_decoder_loss: 0.1649 - val_energy_loss: 0.1302\n",
      "Epoch 256/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2225 - decoder_loss: 0.1654 - energy_loss: 0.0571 - val_loss: 0.2900 - val_decoder_loss: 0.1657 - val_energy_loss: 0.1243\n",
      "Epoch 257/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2211 - decoder_loss: 0.1651 - energy_loss: 0.0560 - val_loss: 0.2806 - val_decoder_loss: 0.1646 - val_energy_loss: 0.1160\n",
      "Epoch 258/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2206 - decoder_loss: 0.1650 - energy_loss: 0.0556 - val_loss: 0.2771 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1124\n",
      "Epoch 259/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2206 - decoder_loss: 0.1651 - energy_loss: 0.0555 - val_loss: 0.2818 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1178\n",
      "Epoch 260/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2210 - decoder_loss: 0.1651 - energy_loss: 0.0559 - val_loss: 0.2836 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1200\n",
      "Epoch 261/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2203 - decoder_loss: 0.1649 - energy_loss: 0.0554 - val_loss: 0.2785 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1148\n",
      "Epoch 262/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2210 - decoder_loss: 0.1652 - energy_loss: 0.0558 - val_loss: 0.2894 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1255\n",
      "Epoch 263/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2205 - decoder_loss: 0.1650 - energy_loss: 0.0555 - val_loss: 0.2857 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1214\n",
      "Epoch 264/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2201 - decoder_loss: 0.1648 - energy_loss: 0.0553 - val_loss: 0.2818 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1173\n",
      "Epoch 265/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2209 - decoder_loss: 0.1651 - energy_loss: 0.0558 - val_loss: 0.3013 - val_decoder_loss: 0.1648 - val_energy_loss: 0.1365\n",
      "Epoch 266/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2203 - decoder_loss: 0.1649 - energy_loss: 0.0554 - val_loss: 0.2830 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1187\n",
      "Epoch 267/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2212 - decoder_loss: 0.1649 - energy_loss: 0.0563 - val_loss: 0.2755 - val_decoder_loss: 0.1646 - val_energy_loss: 0.1109\n",
      "Epoch 268/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2202 - decoder_loss: 0.1647 - energy_loss: 0.0555 - val_loss: 0.2754 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1121\n",
      "Epoch 269/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2206 - decoder_loss: 0.1651 - energy_loss: 0.0555 - val_loss: 0.2824 - val_decoder_loss: 0.1657 - val_energy_loss: 0.1168\n",
      "Epoch 270/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2205 - decoder_loss: 0.1649 - energy_loss: 0.0556 - val_loss: 0.2831 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1194\n",
      "Epoch 271/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2194 - decoder_loss: 0.1645 - energy_loss: 0.0548 - val_loss: 0.2763 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1126\n",
      "Epoch 272/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2198 - decoder_loss: 0.1648 - energy_loss: 0.0549 - val_loss: 0.2834 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1191\n",
      "Epoch 273/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2220 - decoder_loss: 0.1668 - energy_loss: 0.0551 - val_loss: 0.2770 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1133\n",
      "Epoch 274/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2201 - decoder_loss: 0.1645 - energy_loss: 0.0556 - val_loss: 0.2817 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1180\n",
      "Epoch 275/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2197 - decoder_loss: 0.1646 - energy_loss: 0.0551 - val_loss: 0.2834 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1193\n",
      "Epoch 276/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2200 - decoder_loss: 0.1647 - energy_loss: 0.0552 - val_loss: 0.2815 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1178\n",
      "Epoch 277/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2196 - decoder_loss: 0.1647 - energy_loss: 0.0549 - val_loss: 0.2782 - val_decoder_loss: 0.1646 - val_energy_loss: 0.1136\n",
      "Epoch 278/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2198 - decoder_loss: 0.1646 - energy_loss: 0.0552 - val_loss: 0.2792 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1155\n",
      "Epoch 279/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2200 - decoder_loss: 0.1652 - energy_loss: 0.0548 - val_loss: 0.2888 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1248\n",
      "Epoch 280/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2202 - decoder_loss: 0.1646 - energy_loss: 0.0556 - val_loss: 0.2831 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1196\n",
      "Epoch 281/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2191 - decoder_loss: 0.1645 - energy_loss: 0.0547 - val_loss: 0.2805 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1165\n",
      "Epoch 282/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2190 - decoder_loss: 0.1645 - energy_loss: 0.0545 - val_loss: 0.2868 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1228\n",
      "Epoch 283/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2190 - decoder_loss: 0.1646 - energy_loss: 0.0544 - val_loss: 0.2825 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1189\n",
      "Epoch 284/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2199 - decoder_loss: 0.1648 - energy_loss: 0.0551 - val_loss: 0.2811 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1177\n",
      "Epoch 285/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2187 - decoder_loss: 0.1646 - energy_loss: 0.0541 - val_loss: 0.2778 - val_decoder_loss: 0.1627 - val_energy_loss: 0.1151\n",
      "Epoch 286/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2190 - decoder_loss: 0.1644 - energy_loss: 0.0546 - val_loss: 0.2797 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1158\n",
      "Epoch 287/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2202 - decoder_loss: 0.1651 - energy_loss: 0.0552 - val_loss: 0.2712 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2194 - decoder_loss: 0.1644 - energy_loss: 0.0551 - val_loss: 0.2896 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1260\n",
      "Epoch 289/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2190 - decoder_loss: 0.1645 - energy_loss: 0.0545 - val_loss: 0.2751 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1117\n",
      "Epoch 290/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2193 - decoder_loss: 0.1646 - energy_loss: 0.0547 - val_loss: 0.2803 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1163\n",
      "Epoch 291/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2190 - decoder_loss: 0.1643 - energy_loss: 0.0546 - val_loss: 0.2897 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1261\n",
      "Epoch 292/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2192 - decoder_loss: 0.1648 - energy_loss: 0.0544 - val_loss: 0.2882 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1245\n",
      "Epoch 293/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2189 - decoder_loss: 0.1644 - energy_loss: 0.0544 - val_loss: 0.2843 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1204\n",
      "Epoch 294/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2188 - decoder_loss: 0.1644 - energy_loss: 0.0544 - val_loss: 0.2861 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1216\n",
      "Epoch 295/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1643 - energy_loss: 0.0540 - val_loss: 0.2739 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1096\n",
      "Epoch 296/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2188 - decoder_loss: 0.1645 - energy_loss: 0.0544 - val_loss: 0.2937 - val_decoder_loss: 0.1630 - val_energy_loss: 0.1308\n",
      "Epoch 297/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1643 - energy_loss: 0.0540 - val_loss: 0.2721 - val_decoder_loss: 0.1631 - val_energy_loss: 0.1089\n",
      "Epoch 298/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2181 - decoder_loss: 0.1643 - energy_loss: 0.0538 - val_loss: 0.2822 - val_decoder_loss: 0.1648 - val_energy_loss: 0.1173\n",
      "Epoch 299/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2185 - decoder_loss: 0.1644 - energy_loss: 0.0541 - val_loss: 0.2869 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1232\n",
      "Epoch 300/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1645 - energy_loss: 0.0538 - val_loss: 0.2819 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1183\n",
      "Epoch 301/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2188 - decoder_loss: 0.1646 - energy_loss: 0.0543 - val_loss: 0.2883 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1236\n",
      "Epoch 302/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2184 - decoder_loss: 0.1646 - energy_loss: 0.0539 - val_loss: 0.2791 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1156\n",
      "Epoch 303/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2191 - decoder_loss: 0.1644 - energy_loss: 0.0547 - val_loss: 0.2764 - val_decoder_loss: 0.1632 - val_energy_loss: 0.1132\n",
      "Epoch 304/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2200 - decoder_loss: 0.1660 - energy_loss: 0.0540 - val_loss: 0.2862 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1225\n",
      "Epoch 305/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2179 - decoder_loss: 0.1642 - energy_loss: 0.0537 - val_loss: 0.2789 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1147\n",
      "Epoch 306/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2181 - decoder_loss: 0.1643 - energy_loss: 0.0538 - val_loss: 0.2820 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1174\n",
      "Epoch 307/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2192 - decoder_loss: 0.1646 - energy_loss: 0.0546 - val_loss: 0.2712 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1078\n",
      "Epoch 308/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2181 - decoder_loss: 0.1642 - energy_loss: 0.0539 - val_loss: 0.2883 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1245\n",
      "Epoch 309/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2184 - decoder_loss: 0.1643 - energy_loss: 0.0541 - val_loss: 0.2792 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1155\n",
      "Epoch 310/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2180 - decoder_loss: 0.1642 - energy_loss: 0.0538 - val_loss: 0.2854 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1216\n",
      "Epoch 311/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2168 - decoder_loss: 0.1641 - energy_loss: 0.0527 - val_loss: 0.2969 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1329\n",
      "Epoch 312/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2186 - decoder_loss: 0.1646 - energy_loss: 0.0540 - val_loss: 0.2836 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1200\n",
      "Epoch 313/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2182 - decoder_loss: 0.1645 - energy_loss: 0.0536 - val_loss: 0.2743 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1110\n",
      "Epoch 314/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2175 - decoder_loss: 0.1644 - energy_loss: 0.0531 - val_loss: 0.2808 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1172\n",
      "Epoch 315/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2179 - decoder_loss: 0.1643 - energy_loss: 0.0536 - val_loss: 0.2702 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1065\n",
      "Epoch 316/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2177 - decoder_loss: 0.1643 - energy_loss: 0.0535 - val_loss: 0.2797 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1155\n",
      "Epoch 317/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2179 - decoder_loss: 0.1644 - energy_loss: 0.0534 - val_loss: 0.3017 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1379\n",
      "Epoch 318/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1649 - energy_loss: 0.0534 - val_loss: 0.2899 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1256\n",
      "Epoch 319/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2177 - decoder_loss: 0.1641 - energy_loss: 0.0536 - val_loss: 0.2835 - val_decoder_loss: 0.1632 - val_energy_loss: 0.1203\n",
      "Epoch 320/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2172 - decoder_loss: 0.1642 - energy_loss: 0.0530 - val_loss: 0.2755 - val_decoder_loss: 0.1630 - val_energy_loss: 0.1125\n",
      "Epoch 321/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2177 - decoder_loss: 0.1643 - energy_loss: 0.0534 - val_loss: 0.2899 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1264\n",
      "Epoch 322/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1645 - energy_loss: 0.0539 - val_loss: 0.2820 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1181\n",
      "Epoch 323/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2180 - decoder_loss: 0.1644 - energy_loss: 0.0536 - val_loss: 0.2859 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1217\n",
      "Epoch 324/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2180 - decoder_loss: 0.1645 - energy_loss: 0.0535 - val_loss: 0.2815 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1178\n",
      "Epoch 325/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2182 - decoder_loss: 0.1646 - energy_loss: 0.0535 - val_loss: 0.2821 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1185\n",
      "Epoch 326/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1644 - energy_loss: 0.0540 - val_loss: 0.2764 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1124\n",
      "Epoch 327/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2174 - decoder_loss: 0.1643 - energy_loss: 0.0531 - val_loss: 0.2758 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1124\n",
      "Epoch 328/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2176 - decoder_loss: 0.1643 - energy_loss: 0.0533 - val_loss: 0.2691 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2177 - decoder_loss: 0.1644 - energy_loss: 0.0534 - val_loss: 0.2886 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1251\n",
      "Epoch 330/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2178 - decoder_loss: 0.1643 - energy_loss: 0.0535 - val_loss: 0.2802 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1162\n",
      "Epoch 331/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1644 - energy_loss: 0.0525 - val_loss: 0.2848 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1208\n",
      "Epoch 332/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2179 - decoder_loss: 0.1651 - energy_loss: 0.0528 - val_loss: 0.2737 - val_decoder_loss: 0.1628 - val_energy_loss: 0.1110\n",
      "Epoch 333/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2183 - decoder_loss: 0.1644 - energy_loss: 0.0539 - val_loss: 0.2772 - val_decoder_loss: 0.1631 - val_energy_loss: 0.1141\n",
      "Epoch 334/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1641 - energy_loss: 0.0528 - val_loss: 0.2799 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1159\n",
      "Epoch 335/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2175 - decoder_loss: 0.1643 - energy_loss: 0.0533 - val_loss: 0.2813 - val_decoder_loss: 0.1646 - val_energy_loss: 0.1167\n",
      "Epoch 336/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2171 - decoder_loss: 0.1642 - energy_loss: 0.0529 - val_loss: 0.2810 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1177\n",
      "Epoch 337/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1643 - energy_loss: 0.0526 - val_loss: 0.2893 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1252\n",
      "Epoch 338/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2172 - decoder_loss: 0.1644 - energy_loss: 0.0528 - val_loss: 0.2820 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1184\n",
      "Epoch 339/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1642 - energy_loss: 0.0527 - val_loss: 0.2806 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1169\n",
      "Epoch 340/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2171 - decoder_loss: 0.1643 - energy_loss: 0.0528 - val_loss: 0.2801 - val_decoder_loss: 0.1650 - val_energy_loss: 0.1151\n",
      "Epoch 341/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2173 - decoder_loss: 0.1643 - energy_loss: 0.0530 - val_loss: 0.2776 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1140\n",
      "Epoch 342/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1644 - energy_loss: 0.0525 - val_loss: 0.2845 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1207\n",
      "Epoch 343/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2177 - decoder_loss: 0.1643 - energy_loss: 0.0535 - val_loss: 0.2806 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1167\n",
      "Epoch 344/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2185 - decoder_loss: 0.1648 - energy_loss: 0.0536 - val_loss: 0.2855 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1219\n",
      "Epoch 345/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2167 - decoder_loss: 0.1642 - energy_loss: 0.0525 - val_loss: 0.2766 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1122\n",
      "Epoch 346/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2172 - decoder_loss: 0.1643 - energy_loss: 0.0528 - val_loss: 0.2813 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1176\n",
      "Epoch 347/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2170 - decoder_loss: 0.1642 - energy_loss: 0.0528 - val_loss: 0.2808 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1169\n",
      "Epoch 348/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1643 - energy_loss: 0.0527 - val_loss: 0.2732 - val_decoder_loss: 0.1631 - val_energy_loss: 0.1101\n",
      "Epoch 349/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1645 - energy_loss: 0.0525 - val_loss: 0.2863 - val_decoder_loss: 0.1632 - val_energy_loss: 0.1230\n",
      "Epoch 350/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2163 - decoder_loss: 0.1642 - energy_loss: 0.0521 - val_loss: 0.2798 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1161\n",
      "Epoch 351/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1644 - energy_loss: 0.0525 - val_loss: 0.2779 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1134\n",
      "Epoch 352/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2176 - decoder_loss: 0.1642 - energy_loss: 0.0534 - val_loss: 0.2755 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1108\n",
      "Epoch 353/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2167 - decoder_loss: 0.1645 - energy_loss: 0.0522 - val_loss: 0.2865 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1218\n",
      "Epoch 354/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2170 - decoder_loss: 0.1642 - energy_loss: 0.0528 - val_loss: 0.2839 - val_decoder_loss: 0.1625 - val_energy_loss: 0.1214\n",
      "Epoch 355/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2175 - decoder_loss: 0.1644 - energy_loss: 0.0531 - val_loss: 0.2799 - val_decoder_loss: 0.1662 - val_energy_loss: 0.1138\n",
      "Epoch 356/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2172 - decoder_loss: 0.1649 - energy_loss: 0.0523 - val_loss: 0.2908 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1270\n",
      "Epoch 357/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2158 - decoder_loss: 0.1639 - energy_loss: 0.0519 - val_loss: 0.2827 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1185\n",
      "Epoch 358/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2165 - decoder_loss: 0.1643 - energy_loss: 0.0522 - val_loss: 0.2797 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1164\n",
      "Epoch 359/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2170 - decoder_loss: 0.1641 - energy_loss: 0.0529 - val_loss: 0.2849 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1204\n",
      "Epoch 360/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2165 - decoder_loss: 0.1642 - energy_loss: 0.0523 - val_loss: 0.2813 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1169\n",
      "Epoch 361/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2181 - decoder_loss: 0.1648 - energy_loss: 0.0533 - val_loss: 0.2746 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1103\n",
      "Epoch 362/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1642 - energy_loss: 0.0527 - val_loss: 0.2812 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1179\n",
      "Epoch 363/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2169 - decoder_loss: 0.1644 - energy_loss: 0.0525 - val_loss: 0.2782 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1143\n",
      "Epoch 364/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2163 - decoder_loss: 0.1641 - energy_loss: 0.0522 - val_loss: 0.2820 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1177\n",
      "Epoch 365/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2167 - decoder_loss: 0.1643 - energy_loss: 0.0524 - val_loss: 0.2740 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1093\n",
      "Epoch 366/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2161 - decoder_loss: 0.1640 - energy_loss: 0.0521 - val_loss: 0.2771 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1131\n",
      "Epoch 367/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2159 - decoder_loss: 0.1642 - energy_loss: 0.0517 - val_loss: 0.2889 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1244\n",
      "Epoch 368/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2163 - decoder_loss: 0.1643 - energy_loss: 0.0520 - val_loss: 0.2786 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1145\n",
      "Epoch 369/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2163 - decoder_loss: 0.1643 - energy_loss: 0.0520 - val_loss: 0.2736 - val_decoder_loss: 0.1650 - val_energy_loss: 0.1086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2167 - decoder_loss: 0.1643 - energy_loss: 0.0524 - val_loss: 0.2778 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1141\n",
      "Epoch 371/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2171 - decoder_loss: 0.1644 - energy_loss: 0.0527 - val_loss: 0.2811 - val_decoder_loss: 0.1648 - val_energy_loss: 0.1163\n",
      "Epoch 372/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2160 - decoder_loss: 0.1644 - energy_loss: 0.0516 - val_loss: 0.2821 - val_decoder_loss: 0.1646 - val_energy_loss: 0.1174\n",
      "Epoch 373/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2167 - decoder_loss: 0.1642 - energy_loss: 0.0525 - val_loss: 0.2813 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1170\n",
      "Epoch 374/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2166 - decoder_loss: 0.1642 - energy_loss: 0.0524 - val_loss: 0.2812 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1168\n",
      "Epoch 375/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2159 - decoder_loss: 0.1644 - energy_loss: 0.0515 - val_loss: 0.2803 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1168\n",
      "Epoch 376/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2161 - decoder_loss: 0.1644 - energy_loss: 0.0518 - val_loss: 0.2920 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1284\n",
      "Epoch 377/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2161 - decoder_loss: 0.1643 - energy_loss: 0.0518 - val_loss: 0.2885 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1249\n",
      "Epoch 378/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2160 - decoder_loss: 0.1641 - energy_loss: 0.0519 - val_loss: 0.2792 - val_decoder_loss: 0.1649 - val_energy_loss: 0.1143\n",
      "Epoch 379/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2185 - decoder_loss: 0.1666 - energy_loss: 0.0519 - val_loss: 0.2804 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1157\n",
      "Epoch 380/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2159 - decoder_loss: 0.1641 - energy_loss: 0.0517 - val_loss: 0.2849 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1209\n",
      "Epoch 381/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2164 - decoder_loss: 0.1641 - energy_loss: 0.0523 - val_loss: 0.2829 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1193\n",
      "Epoch 382/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2153 - decoder_loss: 0.1641 - energy_loss: 0.0512 - val_loss: 0.2767 - val_decoder_loss: 0.1627 - val_energy_loss: 0.1140\n",
      "Epoch 383/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2157 - decoder_loss: 0.1642 - energy_loss: 0.0515 - val_loss: 0.2870 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1241\n",
      "Epoch 384/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2156 - decoder_loss: 0.1642 - energy_loss: 0.0514 - val_loss: 0.2837 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1194\n",
      "Epoch 385/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2161 - decoder_loss: 0.1643 - energy_loss: 0.0518 - val_loss: 0.2783 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1150\n",
      "Epoch 386/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2157 - decoder_loss: 0.1642 - energy_loss: 0.0515 - val_loss: 0.2781 - val_decoder_loss: 0.1653 - val_energy_loss: 0.1127\n",
      "Epoch 387/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2151 - decoder_loss: 0.1642 - energy_loss: 0.0509 - val_loss: 0.2811 - val_decoder_loss: 0.1632 - val_energy_loss: 0.1179\n",
      "Epoch 388/500\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.2153 - decoder_loss: 0.1642 - energy_loss: 0.0510 - val_loss: 0.2723 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1094\n",
      "Epoch 389/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2159 - decoder_loss: 0.1643 - energy_loss: 0.0516 - val_loss: 0.2769 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1131\n",
      "Epoch 390/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2163 - decoder_loss: 0.1642 - energy_loss: 0.0522 - val_loss: 0.2913 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1279\n",
      "Epoch 391/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2162 - decoder_loss: 0.1643 - energy_loss: 0.0519 - val_loss: 0.2782 - val_decoder_loss: 0.1632 - val_energy_loss: 0.1150\n",
      "Epoch 392/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2157 - decoder_loss: 0.1640 - energy_loss: 0.0517 - val_loss: 0.2831 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1188\n",
      "Epoch 393/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2157 - decoder_loss: 0.1642 - energy_loss: 0.0515 - val_loss: 0.2803 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1169\n",
      "Epoch 394/500\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.2178 - decoder_loss: 0.1661 - energy_loss: 0.0517 - val_loss: 0.2789 - val_decoder_loss: 0.1630 - val_energy_loss: 0.1159\n",
      "Epoch 395/500\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.2154 - decoder_loss: 0.1641 - energy_loss: 0.0513 - val_loss: 0.2811 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1174\n",
      "Epoch 396/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2160 - decoder_loss: 0.1643 - energy_loss: 0.0518 - val_loss: 0.2803 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1163\n",
      "Epoch 397/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2163 - decoder_loss: 0.1642 - energy_loss: 0.0520 - val_loss: 0.2940 - val_decoder_loss: 0.1651 - val_energy_loss: 0.1289\n",
      "Epoch 398/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2154 - decoder_loss: 0.1641 - energy_loss: 0.0513 - val_loss: 0.2829 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1195\n",
      "Epoch 399/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2154 - decoder_loss: 0.1641 - energy_loss: 0.0514 - val_loss: 0.2813 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1175\n",
      "Epoch 400/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1643 - energy_loss: 0.0505 - val_loss: 0.2795 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1162\n",
      "Epoch 401/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2158 - decoder_loss: 0.1642 - energy_loss: 0.0516 - val_loss: 0.2780 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1147\n",
      "Epoch 402/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2150 - decoder_loss: 0.1643 - energy_loss: 0.0507 - val_loss: 0.2777 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1135\n",
      "Epoch 403/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2156 - decoder_loss: 0.1643 - energy_loss: 0.0513 - val_loss: 0.2749 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1109\n",
      "Epoch 404/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1640 - energy_loss: 0.0507 - val_loss: 0.2838 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1197\n",
      "Epoch 405/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2155 - decoder_loss: 0.1643 - energy_loss: 0.0513 - val_loss: 0.2760 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1124\n",
      "Epoch 406/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2151 - decoder_loss: 0.1641 - energy_loss: 0.0509 - val_loss: 0.2808 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1169\n",
      "Epoch 407/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2156 - decoder_loss: 0.1642 - energy_loss: 0.0513 - val_loss: 0.2760 - val_decoder_loss: 0.1631 - val_energy_loss: 0.1128\n",
      "Epoch 408/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2150 - decoder_loss: 0.1642 - energy_loss: 0.0508 - val_loss: 0.2808 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1168\n",
      "Epoch 409/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1642 - energy_loss: 0.0506 - val_loss: 0.2761 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1122\n",
      "Epoch 410/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2159 - decoder_loss: 0.1642 - energy_loss: 0.0517 - val_loss: 0.2798 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 411/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2154 - decoder_loss: 0.1643 - energy_loss: 0.0511 - val_loss: 0.2957 - val_decoder_loss: 0.1664 - val_energy_loss: 0.1293\n",
      "Epoch 412/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2150 - decoder_loss: 0.1642 - energy_loss: 0.0508 - val_loss: 0.2768 - val_decoder_loss: 0.1631 - val_energy_loss: 0.1138\n",
      "Epoch 413/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2149 - decoder_loss: 0.1642 - energy_loss: 0.0507 - val_loss: 0.2913 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1269\n",
      "Epoch 414/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2151 - decoder_loss: 0.1640 - energy_loss: 0.0510 - val_loss: 0.2804 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1163\n",
      "Epoch 415/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2159 - decoder_loss: 0.1643 - energy_loss: 0.0516 - val_loss: 0.2738 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1103\n",
      "Epoch 416/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2143 - decoder_loss: 0.1641 - energy_loss: 0.0502 - val_loss: 0.2852 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1208\n",
      "Epoch 417/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1642 - energy_loss: 0.0506 - val_loss: 0.2763 - val_decoder_loss: 0.1647 - val_energy_loss: 0.1117\n",
      "Epoch 418/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2154 - decoder_loss: 0.1642 - energy_loss: 0.0512 - val_loss: 0.2787 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1145\n",
      "Epoch 419/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2147 - decoder_loss: 0.1642 - energy_loss: 0.0505 - val_loss: 0.2760 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1124\n",
      "Epoch 420/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2147 - decoder_loss: 0.1643 - energy_loss: 0.0504 - val_loss: 0.2784 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1140\n",
      "Epoch 421/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2178 - decoder_loss: 0.1672 - energy_loss: 0.0505 - val_loss: 0.2849 - val_decoder_loss: 0.1660 - val_energy_loss: 0.1189\n",
      "Epoch 422/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2149 - decoder_loss: 0.1644 - energy_loss: 0.0505 - val_loss: 0.2739 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1103\n",
      "Epoch 423/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1639 - energy_loss: 0.0508 - val_loss: 0.2837 - val_decoder_loss: 0.1628 - val_energy_loss: 0.1209\n",
      "Epoch 424/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1642 - energy_loss: 0.0506 - val_loss: 0.2796 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1157\n",
      "Epoch 425/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2149 - decoder_loss: 0.1640 - energy_loss: 0.0509 - val_loss: 0.2901 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1264\n",
      "Epoch 426/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1643 - energy_loss: 0.0506 - val_loss: 0.2816 - val_decoder_loss: 0.1650 - val_energy_loss: 0.1167\n",
      "Epoch 427/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2153 - decoder_loss: 0.1645 - energy_loss: 0.0508 - val_loss: 0.2740 - val_decoder_loss: 0.1627 - val_energy_loss: 0.1113\n",
      "Epoch 428/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1641 - energy_loss: 0.0500 - val_loss: 0.2883 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1238\n",
      "Epoch 429/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2142 - decoder_loss: 0.1642 - energy_loss: 0.0501 - val_loss: 0.2789 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1160\n",
      "Epoch 430/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2146 - decoder_loss: 0.1642 - energy_loss: 0.0504 - val_loss: 0.2773 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1140\n",
      "Epoch 431/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1642 - energy_loss: 0.0499 - val_loss: 0.2839 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1203\n",
      "Epoch 432/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1641 - energy_loss: 0.0500 - val_loss: 0.2884 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1249\n",
      "Epoch 433/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2147 - decoder_loss: 0.1642 - energy_loss: 0.0505 - val_loss: 0.2788 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1153\n",
      "Epoch 434/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2151 - decoder_loss: 0.1643 - energy_loss: 0.0508 - val_loss: 0.2767 - val_decoder_loss: 0.1632 - val_energy_loss: 0.1134\n",
      "Epoch 435/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2139 - decoder_loss: 0.1640 - energy_loss: 0.0499 - val_loss: 0.2807 - val_decoder_loss: 0.1652 - val_energy_loss: 0.1155\n",
      "Epoch 436/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2161 - decoder_loss: 0.1652 - energy_loss: 0.0510 - val_loss: 0.2733 - val_decoder_loss: 0.1631 - val_energy_loss: 0.1102\n",
      "Epoch 437/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2171 - decoder_loss: 0.1665 - energy_loss: 0.0506 - val_loss: 0.2890 - val_decoder_loss: 0.1708 - val_energy_loss: 0.1181\n",
      "Epoch 438/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1646 - energy_loss: 0.0502 - val_loss: 0.2803 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1162\n",
      "Epoch 439/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2146 - decoder_loss: 0.1640 - energy_loss: 0.0507 - val_loss: 0.2939 - val_decoder_loss: 0.1648 - val_energy_loss: 0.1291\n",
      "Epoch 440/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2151 - decoder_loss: 0.1641 - energy_loss: 0.0510 - val_loss: 0.2788 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1147\n",
      "Epoch 441/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2145 - decoder_loss: 0.1640 - energy_loss: 0.0505 - val_loss: 0.2889 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1254\n",
      "Epoch 442/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2139 - decoder_loss: 0.1642 - energy_loss: 0.0497 - val_loss: 0.2994 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1352\n",
      "Epoch 443/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2144 - decoder_loss: 0.1641 - energy_loss: 0.0503 - val_loss: 0.2809 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1172\n",
      "Epoch 444/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1643 - energy_loss: 0.0505 - val_loss: 0.2731 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1093\n",
      "Epoch 445/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2149 - decoder_loss: 0.1642 - energy_loss: 0.0506 - val_loss: 0.2791 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1162\n",
      "Epoch 446/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2142 - decoder_loss: 0.1643 - energy_loss: 0.0499 - val_loss: 0.2940 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1305\n",
      "Epoch 447/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2151 - decoder_loss: 0.1643 - energy_loss: 0.0509 - val_loss: 0.2778 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1143\n",
      "Epoch 448/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2140 - decoder_loss: 0.1641 - energy_loss: 0.0498 - val_loss: 0.2822 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1188\n",
      "Epoch 449/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2144 - decoder_loss: 0.1644 - energy_loss: 0.0499 - val_loss: 0.2813 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1176\n",
      "Epoch 450/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1642 - energy_loss: 0.0499 - val_loss: 0.2735 - val_decoder_loss: 0.1641 - val_energy_loss: 0.1094\n",
      "Epoch 451/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2145 - decoder_loss: 0.1643 - energy_loss: 0.0502 - val_loss: 0.2855 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2142 - decoder_loss: 0.1642 - energy_loss: 0.0500 - val_loss: 0.2807 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1174\n",
      "Epoch 453/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1641 - energy_loss: 0.0500 - val_loss: 0.2786 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1146\n",
      "Epoch 454/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2143 - decoder_loss: 0.1644 - energy_loss: 0.0499 - val_loss: 0.2837 - val_decoder_loss: 0.1643 - val_energy_loss: 0.1194\n",
      "Epoch 455/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2140 - decoder_loss: 0.1641 - energy_loss: 0.0499 - val_loss: 0.2798 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1159\n",
      "Epoch 456/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1642 - energy_loss: 0.0506 - val_loss: 0.2749 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1115\n",
      "Epoch 457/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1642 - energy_loss: 0.0499 - val_loss: 0.2840 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1206\n",
      "Epoch 458/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1642 - energy_loss: 0.0499 - val_loss: 0.2767 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1125\n",
      "Epoch 459/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2147 - decoder_loss: 0.1642 - energy_loss: 0.0505 - val_loss: 0.2897 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1263\n",
      "Epoch 460/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2144 - decoder_loss: 0.1642 - energy_loss: 0.0502 - val_loss: 0.2891 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1246\n",
      "Epoch 461/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2135 - decoder_loss: 0.1640 - energy_loss: 0.0495 - val_loss: 0.2869 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1230\n",
      "Epoch 462/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1641 - energy_loss: 0.0496 - val_loss: 0.2816 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1178\n",
      "Epoch 463/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2143 - decoder_loss: 0.1643 - energy_loss: 0.0500 - val_loss: 0.2715 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1082\n",
      "Epoch 464/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1642 - energy_loss: 0.0498 - val_loss: 0.2805 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1171\n",
      "Epoch 465/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2148 - decoder_loss: 0.1643 - energy_loss: 0.0505 - val_loss: 0.2910 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1271\n",
      "Epoch 466/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2141 - decoder_loss: 0.1641 - energy_loss: 0.0500 - val_loss: 0.2782 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1149\n",
      "Epoch 467/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2137 - decoder_loss: 0.1642 - energy_loss: 0.0495 - val_loss: 0.2750 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1113\n",
      "Epoch 468/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2145 - decoder_loss: 0.1641 - energy_loss: 0.0504 - val_loss: 0.2815 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1182\n",
      "Epoch 469/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2139 - decoder_loss: 0.1643 - energy_loss: 0.0496 - val_loss: 0.2778 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1139\n",
      "Epoch 470/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2138 - decoder_loss: 0.1641 - energy_loss: 0.0497 - val_loss: 0.2779 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1140\n",
      "Epoch 471/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2143 - decoder_loss: 0.1644 - energy_loss: 0.0499 - val_loss: 0.2907 - val_decoder_loss: 0.1630 - val_energy_loss: 0.1276\n",
      "Epoch 472/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2134 - decoder_loss: 0.1642 - energy_loss: 0.0492 - val_loss: 0.2820 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1191\n",
      "Epoch 473/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2135 - decoder_loss: 0.1640 - energy_loss: 0.0495 - val_loss: 0.2832 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1203\n",
      "Epoch 474/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2137 - decoder_loss: 0.1642 - energy_loss: 0.0495 - val_loss: 0.2791 - val_decoder_loss: 0.1652 - val_energy_loss: 0.1140\n",
      "Epoch 475/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1642 - energy_loss: 0.0494 - val_loss: 0.2862 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1228\n",
      "Epoch 476/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1643 - energy_loss: 0.0493 - val_loss: 0.2799 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1161\n",
      "Epoch 477/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2131 - decoder_loss: 0.1641 - energy_loss: 0.0489 - val_loss: 0.2797 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1158\n",
      "Epoch 478/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1642 - energy_loss: 0.0493 - val_loss: 0.2871 - val_decoder_loss: 0.1640 - val_energy_loss: 0.1231\n",
      "Epoch 479/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2139 - decoder_loss: 0.1643 - energy_loss: 0.0496 - val_loss: 0.2768 - val_decoder_loss: 0.1630 - val_energy_loss: 0.1139\n",
      "Epoch 480/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2147 - decoder_loss: 0.1650 - energy_loss: 0.0498 - val_loss: 0.2779 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1138\n",
      "Epoch 481/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2130 - decoder_loss: 0.1639 - energy_loss: 0.0491 - val_loss: 0.2803 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1170\n",
      "Epoch 482/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2127 - decoder_loss: 0.1639 - energy_loss: 0.0488 - val_loss: 0.2771 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1129\n",
      "Epoch 483/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2133 - decoder_loss: 0.1642 - energy_loss: 0.0491 - val_loss: 0.2758 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1114\n",
      "Epoch 484/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2140 - decoder_loss: 0.1642 - energy_loss: 0.0498 - val_loss: 0.2858 - val_decoder_loss: 0.1634 - val_energy_loss: 0.1224\n",
      "Epoch 485/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1642 - energy_loss: 0.0494 - val_loss: 0.2739 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1101\n",
      "Epoch 486/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2150 - decoder_loss: 0.1661 - energy_loss: 0.0490 - val_loss: 0.2800 - val_decoder_loss: 0.1636 - val_energy_loss: 0.1164\n",
      "Epoch 487/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2134 - decoder_loss: 0.1640 - energy_loss: 0.0495 - val_loss: 0.2854 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1216\n",
      "Epoch 488/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2132 - decoder_loss: 0.1641 - energy_loss: 0.0491 - val_loss: 0.2747 - val_decoder_loss: 0.1627 - val_energy_loss: 0.1120\n",
      "Epoch 489/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2133 - decoder_loss: 0.1640 - energy_loss: 0.0493 - val_loss: 0.2750 - val_decoder_loss: 0.1633 - val_energy_loss: 0.1117\n",
      "Epoch 490/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2130 - decoder_loss: 0.1640 - energy_loss: 0.0490 - val_loss: 0.2745 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1107\n",
      "Epoch 491/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2138 - decoder_loss: 0.1643 - energy_loss: 0.0495 - val_loss: 0.2774 - val_decoder_loss: 0.1644 - val_energy_loss: 0.1130\n",
      "Epoch 492/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1641 - energy_loss: 0.0495 - val_loss: 0.2859 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 493/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2134 - decoder_loss: 0.1642 - energy_loss: 0.0492 - val_loss: 0.2765 - val_decoder_loss: 0.1628 - val_energy_loss: 0.1137\n",
      "Epoch 494/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2133 - decoder_loss: 0.1640 - energy_loss: 0.0493 - val_loss: 0.2851 - val_decoder_loss: 0.1639 - val_energy_loss: 0.1212\n",
      "Epoch 495/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2133 - decoder_loss: 0.1640 - energy_loss: 0.0493 - val_loss: 0.2887 - val_decoder_loss: 0.1645 - val_energy_loss: 0.1241\n",
      "Epoch 496/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2140 - decoder_loss: 0.1643 - energy_loss: 0.0498 - val_loss: 0.2892 - val_decoder_loss: 0.1638 - val_energy_loss: 0.1254\n",
      "Epoch 497/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2131 - decoder_loss: 0.1640 - energy_loss: 0.0491 - val_loss: 0.2797 - val_decoder_loss: 0.1637 - val_energy_loss: 0.1159\n",
      "Epoch 498/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2131 - decoder_loss: 0.1641 - energy_loss: 0.0490 - val_loss: 0.2806 - val_decoder_loss: 0.1642 - val_energy_loss: 0.1164\n",
      "Epoch 499/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2136 - decoder_loss: 0.1641 - energy_loss: 0.0495 - val_loss: 0.2796 - val_decoder_loss: 0.1629 - val_energy_loss: 0.1166\n",
      "Epoch 500/500\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.2139 - decoder_loss: 0.1643 - energy_loss: 0.0496 - val_loss: 0.2839 - val_decoder_loss: 0.1635 - val_energy_loss: 0.1205\n"
     ]
    }
   ],
   "source": [
    "hist = full_model.fit(x_train_features,\n",
    "                 {'energy': y_train, 'decoder': x_train_features},\n",
    "                 epochs=500,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 validation_data=(x_test_features, {'energy': y_test, 'decoder': x_test_features}),\n",
    "                 verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Features (InputLayer)        [(None, 62)]              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 32)                2016      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 6,450\n",
      "Trainable params: 6,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 62)                2046      \n",
      "=================================================================\n",
      "Total params: 2,590\n",
      "Trainable params: 2,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first argument to `Layer.call` must always be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-a9b8d43e16e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-135-f690878a8d14>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(latent_dim)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mLATENT_INPUTS\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'energy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     model.compile(loss={'energy': 'mse',\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m       raise ValueError(\n\u001b[1;32m--> 800\u001b[1;33m           'The first argument to `Layer.call` must always be passed.')\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[0mcall_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
     ]
    }
   ],
   "source": [
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "def model():\n",
    "    #encoder\n",
    "    features = Input(shape=(62,),name='Features')\n",
    "    X = Dense(62, activation = 'relu')(features)\n",
    "    X1 = Dense(32, activation = 'relu')(X)\n",
    "    encoded = Dense(latent_dim, activation = 'relu', name = 'encoder')(X1)\n",
    "    \n",
    "    #difference model\n",
    "    X2 = Dense(32,activation='relu')(encoded)\n",
    "    X3 = Dropout(0.2)(X2)\n",
    "    X4 = Dense(6,activation='relu')(X3)\n",
    "    output = Dense(1, activation='linear', name = 'energy')(X4)\n",
    "    \n",
    "    #decoder \n",
    "    X5 = Dense(32, activation = 'relu')(encoded)\n",
    "    decoded = Dense(62, activation = 'linear', name = 'autoencoder')(X5)\n",
    "    \n",
    "    model = Model(inputs = features, outputs = [output, decoded])\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Features (InputLayer)           [(None, 62)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 62)           3906        Features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2016        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Dense)                 (None, 16)           528         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           544         encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            198         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           544         encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "energy (Dense)                  (None, 1)            7           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "autoencoder (Dense)             (None, 62)           2046        dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,789\n",
      "Trainable params: 9,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss={'energy': 'mse', \n",
    "                    'autoencoder': 'mse'},\n",
    "              optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:183 __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:63 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:587 map_to_output_names\n        struct.keys(), output_names))\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['autoencoder']). Expected: ['decoder', 'energy']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-327723447c11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'energy'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'autoencoder'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_test_features\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                  verbose = 1)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    625\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 506\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2446\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:183 __call__\n        y_true = self._conform_to_outputs(y_pred, y_true)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:63 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    C:\\Users\\keith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:587 map_to_output_names\n        struct.keys(), output_names))\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['autoencoder']). Expected: ['decoder', 'energy']\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_features,\n",
    "                 {'energy': y_train, 'autoencoder': x_train_features},\n",
    "                 epochs=500,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 validation_data=(x_test_features, {'energy': y_test, 'autoencoder': x_test_features}),\n",
    "                 verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.test_util.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}